# -*- coding: utf-8 -*-
"""18_08_12.50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y7KebUjounAjF70n1SkIOddj1NnfQQKs
"""

from google.colab import drive
# Mount Google Drive
drive.mount('/content/drive')

!pip install nnunetv2 SimpleITK nibabel pandas numpy matplotlib

import os
# Set nnUNet paths
base_path = '/content/drive/MyDrive/pancreas_project'
os.environ['nnUNet_raw'] = f'{base_path}/nnUNet_raw'
os.environ['nnUNet_preprocessed'] = f'{base_path}/nnUNet_preprocessed'
os.environ['nnUNet_results'] = f'{base_path}/nnUNet_results'
print("Paths set successfully!")

!pip install -e

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries
import torch
import numpy as np
from pathlib import Path
import nibabel as nib
import shutil
import json
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import torch.nn as nn
from torch.cuda import amp
from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer
from nnunetv2.paths import nnUNet_preprocessed, nnUNet_results
from nnunetv2.utilities.default_n_proc_DA import get_allowed_n_proc_DA

# %matplotlib inline



# Check GPU availability
!nvidia-smi

print("\nPyTorch GPU Status:")
print("GPU Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU Device Name:", torch.cuda.get_device_name(0))
else:
    print("No GPU found!")

"""OPTIONAL"""

import os

base_path = "/content/drive/MyDrive/pancreas_project"

for root, dirs, files in os.walk(base_path):
    level = root.replace(base_path, "").count(os.sep)
    indent = " " * 4 * level
    print(f"{indent}{os.path.basename(root)}/")
    sub_indent = " " * 4 * (level + 1)
    for f in files[:5]:  # show up to 5 files per folder for brevity
        print(f"{sub_indent}{f}")

import os
import json
import shutil

base_path = "/content/drive/MyDrive/pancreas_project"
dataset_json_path = os.path.join(base_path, "nnUNet_raw", "Dataset001_PancreasCancer", "dataset.json")
nnunet_raw_data_path = os.path.join(base_path, "nnUNet_raw", "Dataset001_PancreasCancer")

# Create necessary directories if they don't exist
os.makedirs(os.path.join(nnunet_raw_data_path, "imagesTr"), exist_ok=True)
os.makedirs(os.path.join(nnunet_raw_data_path, "labelsTr"), exist_ok=True)
os.makedirs(os.path.join(nnunet_raw_data_path, "imagesTs"), exist_ok=True)
os.makedirs(os.path.join(nnunet_raw_data_path, "imagesVal"), exist_ok=True) # Create validation image directory
os.makedirs(os.path.join(nnunet_raw_data_path, "labelsVal"), exist_ok=True) # Create validation label directory


# Initialize dataset.json
dataset_json = {
    "name": "Pancreas",
    "description": "Pancreas Segmentation Dataset",
    "tensorImageSize": "3D",
    "channel_names": {"0": "CT"},
    "labels": {"background": 0, "pancreas": 1, "lesion": 2},
    "numTraining": 0,
    "numValidation": 0, # Add validation count
    "numTest": 0,
    "file_ending": ".nii.gz",
    "training": [],
    "validation": [], # Add validation list
    "test": []
}

# Helper function to filter valid NIfTI files
def valid_nifti(file_name):
    return file_name.endswith(".nii.gz") and not file_name.startswith("._")

# -----------------------------
# Process training data
# -----------------------------
train_subfolders = ["subtype0", "subtype1", "subtype2"]
for sub in train_subfolders:
    folder = os.path.join(base_path, "train", sub)
    if os.path.exists(folder):
        for f in os.listdir(folder):
            if valid_nifti(f):
                image_path = os.path.join(folder, f)
                # Construct label path: assuming labels in the same folder with same file name minus _0000 if present
                case_id = f.replace("_0000.nii.gz", "").replace(".nii.gz","")
                label_path_candidate = os.path.join(folder, f"{case_id}.nii.gz")

                if "_0000.nii.gz" in f and os.path.exists(label_path_candidate):
                    shutil.copy(image_path, os.path.join(nnunet_raw_data_path, "imagesTr", f))
                    shutil.copy(label_path_candidate, os.path.join(nnunet_raw_data_path, "labelsTr", f"{case_id}.nii.gz"))
                    dataset_json["training"].append({
                        "image": f"./imagesTr/{f}",
                        "label": f"./labelsTr/{case_id}.nii.gz"
                    })
                elif "_0000.nii.gz" not in f and os.path.exists(os.path.join(folder, f"{case_id}_0000.nii.gz")):
                     # This is likely a label file if its corresponding image_0000 exists
                     pass # We handle the label copying when we process the image_0000 file
                else:
                    print(f"Skipping unexpected file in training folder: {f}")


dataset_json["numTraining"] = len(dataset_json["training"])

# -----------------------------
# Process validation data
# -----------------------------
validation_subfolders = ["subtype0", "subtype1", "subtype2"]
for sub in validation_subfolders:
    folder = os.path.join(base_path, "validation", sub)
    if os.path.exists(folder):
        for f in os.listdir(folder):
            if valid_nifti(f):
                image_path = os.path.join(folder, f)
                case_id = f.replace("_0000.nii.gz", "").replace(".nii.gz","")
                label_path_candidate = os.path.join(folder, f"{case_id}.nii.gz")

                if "_0000.nii.gz" in f and os.path.exists(label_path_candidate):
                    shutil.copy(image_path, os.path.join(nnunet_raw_data_path, "imagesVal", f))
                    shutil.copy(label_path_candidate, os.path.join(nnunet_raw_data_path, "labelsVal", f"{case_id}.nii.gz"))
                    dataset_json["validation"].append({
                        "image": f"./imagesVal/{f}",
                        "label": f"./labelsVal/{case_id}.nii.gz"
                    })
                elif "_0000.nii.gz" not in f and os.path.exists(os.path.join(folder, f"{case_id}_0000.nii.gz")):
                     # This is likely a label file if its corresponding image_0000 exists
                     pass # We handle the label copying when we process the image_0000 file
                else:
                     print(f"Skipping unexpected file in validation folder: {f}")


dataset_json["numValidation"] = len(dataset_json["validation"])


# -----------------------------
# Process test data (flat folder)
# -----------------------------
test_folder = os.path.join(base_path, "test")
if os.path.exists(test_folder):
    for f in os.listdir(test_folder):
        if valid_nifti(f) and "_0000.nii.gz" in f: # Assuming test images follow _0000 convention
            file_path = os.path.join(test_folder, f)
            shutil.copy(file_path, os.path.join(nnunet_raw_data_path, "imagesTs", f))
            dataset_json["test"].append({"image": f"./imagesTs/{f}"})
        elif valid_nifti(f) and "_0000.nii.gz" not in f:
             print(f"Skipping unexpected file in test folder (likely a label): {f}")


dataset_json["numTest"] = len(dataset_json["test"])


# -----------------------------
# Save dataset.json
# -----------------------------
with open(dataset_json_path, "w") as f:
    json.dump(dataset_json, f, indent=4)

print("‚úÖ dataset.json updated successfully!")
print(f"Training cases: {dataset_json['numTraining']}")
print(f"Validation cases: {dataset_json['numValidation']}")
print(f"Test cases: {dataset_json['numTest']}")

def organize_data_for_nnunet():
    base_path = Path('/content/drive/MyDrive/pancreas_project')
    dataset_name = 'Dataset001_PancreasCancer'  # Keep original name
    nnunet_raw_data_path = base_path / 'nnUNet_raw' / dataset_name

    # Create all necessary directories including validation folders
    for folder in ['imagesTr', 'labelsTr', 'imagesTs', 'imagesVal', 'labelsVal']:
        (nnunet_raw_data_path / folder).mkdir(parents=True, exist_ok=True)

    # Create dataset.json
    dataset_json = {
        "name": "Pancreas",
        "description": "Pancreas Segmentation Dataset",
        "tensorImageSize": "3D",
        "channel_names": {
            "0": "CT"
        },
        "labels": {
            "background": 0,
            "pancreas": 1,
            "lesion": 2
        },
        "class_types": {
            "subtype0": 0,
            "subtype1": 1,
            "subtype2": 2
        },
        "numTraining": 0,
        "numValidation": 0,
        "numTest": 0,
        "file_ending": ".nii.gz",
        "training": [],
        "validation": [],
        "test": []
    }

    subtype_mapping = {}

    # Process training data
    for subtype in ['subtype0', 'subtype1', 'subtype2']:
        train_path = base_path / 'data' / 'train' / subtype
        if train_path.exists():
            for file in sorted(train_path.glob('*_0000.nii.gz')):
                case_id = file.stem.split('_0000')[0]
                mask_file = train_path / f"{case_id}.nii.gz"

                if not mask_file.exists():
                    print(f"Warning: Mask file not found for training case '{case_id}'. Skipping...")
                    continue

                try:
                    shutil.copy2(file, nnunet_raw_data_path / 'imagesTr' / file.name)

                    mask = nib.load(mask_file)
                    mask_data = np.round(mask.get_fdata()).astype(np.uint8)
                    mask_data = np.clip(mask_data, 0, 2)
                    new_mask = nib.Nifti1Image(mask_data, mask.affine)
                    nib.save(new_mask, nnunet_raw_data_path / 'labelsTr' / f"{case_id}.nii.gz")

                    dataset_json["training"].append({
                        "image": f"./imagesTr/{file.name}",
                        "label": f"./labelsTr/{case_id}.nii.gz"
                    })

                    subtype_mapping[case_id] = {
                        "subtype": int(subtype[-1]),
                        "split": "train"
                    }

                except Exception as e:
                    print(f"Error processing training case '{case_id}': {e}")

    # Process validation data - store in separate folders
    validation_cases = []
    for subtype in ['subtype0', 'subtype1', 'subtype2']:
        val_path = base_path / 'data' / 'validation' / subtype
        if val_path.exists():
            for file in sorted(val_path.glob('*_0000.nii.gz')):
                case_id = file.stem.split('_0000')[0]
                mask_file = val_path / f"{case_id}.nii.gz"

                if not mask_file.exists():
                    print(f"Warning: Mask file not found for validation case '{case_id}'. Skipping...")
                    continue

                try:
                    # Copy to validation folders
                    shutil.copy2(file, nnunet_raw_data_path / 'imagesVal' / file.name)

                    mask = nib.load(mask_file)
                    mask_data = np.round(mask.get_fdata()).astype(np.uint8)
                    mask_data = np.clip(mask_data, 0, 2)
                    new_mask = nib.Nifti1Image(mask_data, mask.affine)
                    nib.save(new_mask, nnunet_raw_data_path / 'labelsVal' / f"{case_id}.nii.gz")

                    dataset_json["validation"].append({
                        "image": f"./imagesVal/{file.name}",
                        "label": f"./labelsVal/{case_id}.nii.gz"
                    })

                    validation_cases.append(case_id)
                    subtype_mapping[case_id] = {
                        "subtype": int(subtype[-1]),
                        "split": "validation"
                    }

                except Exception as e:
                    print(f"Error processing validation case '{case_id}': {e}")

    # Process test data
    test_path = base_path / 'data' / 'test'
    if test_path.exists():
        for file in sorted(test_path.glob('*_0000.nii.gz')):
            try:
                shutil.copy2(file, nnunet_raw_data_path / 'imagesTs' / file.name)
                dataset_json["test"].append({"image": f"./imagesTs/{file.name}"})
            except Exception as e:
                print(f"Error processing test file '{file}': {e}")

    # Update dataset.json
    dataset_json["numTraining"] = len(dataset_json["training"])
    dataset_json["numValidation"] = len(dataset_json["validation"])
    dataset_json["numTest"] = len(dataset_json["test"])

    # Save dataset.json
    with open(nnunet_raw_data_path / 'dataset.json', 'w') as f:
        json.dump(dataset_json, f, indent=4)

    # Save subtype mapping with validation cases
    with open(nnunet_raw_data_path / 'subtype_mapping.json', 'w') as f:
        json.dump({
            "mapping": subtype_mapping,
            "validation_cases": validation_cases
        }, f, indent=4)


    print("\nData Organization Summary:")
    print(f"Training cases: {dataset_json['numTraining']}")
    print(f"Validation cases: {dataset_json['numValidation']}")
    print(f"Test cases: {dataset_json['numTest']}")

    return nnunet_raw_data_path, subtype_mapping

if __name__ == "__main__":
    nnunet_dataset_path, metadata = organize_data_for_nnunet()

    # Verify the file structure
    print("\nVerifying file structure:")
    print(f"Training images: {len(list((nnunet_dataset_path / 'imagesTr').glob('*.nii.gz')))}")
    print(f"Training labels: {len(list((nnunet_dataset_path / 'labelsTr').glob('*.nii.gz')))}")
    print(f"Validation images: {len(list((nnunet_dataset_path / 'imagesVal').glob('*.nii.gz')))}")
    print(f"Validation labels: {len(list((nnunet_dataset_path / 'labelsVal').glob('*.nii.gz')))}")
    print(f"Test images: {len(list((nnunet_dataset_path / 'imagesTs').glob('*.nii.gz')))}")

#!pip install -e .

# Clone the nnUNet repository
!git clone https://github.com/MIC-DKFZ/nnUNet.git

# Commented out IPython magic to ensure Python compatibility.
# Change directory to the cloned repository
# %cd /content/drive/MyDrive/pancreas_project

# Data preprocessing and planning with ResEncM planner.
!nnUNetv2_plan_and_preprocess -d 1 -pl nnUNetPlannerResEncM

# Clone the nnUNet repository
!git clone https://github.com/MIC-DKFZ/nnUNet.git

# Commented out IPython magic to ensure Python compatibility.

# Change directory to the cloned repository
# %cd $base_path

# Install nnUNet in editable mode
!pip install -e .

!nnUNetv2_train --help

# Training Dataset001 with 3D full resolution using ResEncM planner
!nnUNetv2_train 001 3d_fullres 0 -tr nnUNetTrainer -p nnUNetResEncUNetMPlans --c

"""Running Validation Inference"""

# Using trained model to predict segmentation masks on validation images,
!nnUNetv2_predict -i /content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/imagesVal \
                 -o /content/drive/MyDrive/pancreas_project/nnUNet_results/Dataset001_PancreasCancer/nnUNetTrainer__nnUNetResEncUNetMPlans__3d_fullres/validation_predictions \
                 -d 001 \
                 -c 3d_fullres \
                 -tr nnUNetTrainer \
                 -p nnUNetResEncUNetMPlans \
                 -f 0 \
                 -chk checkpoint_best.pth

"""Running Test inference"""

# Predicting segmentation masks on test images using trained model
!nnUNetv2_predict -i /content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/imagesTs \
                 -o /content/drive/MyDrive/pancreas_project/nnUNet_results/Dataset001_PancreasCancer/test_output \
                 -d 001 \
                 -c 3d_fullres \
                 -tr nnUNetTrainer \
                 -p nnUNetResEncUNetMPlans \
                 -f 0 \
                 -chk checkpoint_best.pth

"""Visualizing Validation Results"""

# File paths
image_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/imagesVal/quiz_0_168_0000.nii.gz"
ground_truth_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/labelsVal/quiz_0_168.nii.gz"
predicted_mask_path = "/content/drive/MyDrive/pancreas_project/nnUNet_results/Dataset001_PancreasCancer/inference_output/quiz_0_168.nii.gz"

# Load the image, ground truth, and predicted mask
image = nib.load(image_path).get_fdata()
ground_truth = nib.load(ground_truth_path).get_fdata()
predicted_mask = nib.load(predicted_mask_path).get_fdata()

# Normalize image to [0, 1] for display
image_min, image_max = np.min(image), np.max(image)
image = (image - image_min) / (image_max - image_min)

# Select a slice with meaningful content
slice_idx = image.shape[2] // 2  # Middle slice
if np.sum(image[:, :, slice_idx]) == 0:  # If slice is empty, find another slice
    slice_idx = next((i for i in range(image.shape[2]) if np.sum(image[:, :, i]) > 0), slice_idx)

print(f"Selected slice index: {slice_idx}")
print(f"Image slice min, max: {np.min(image[:, :, slice_idx])}, {np.max(image[:, :, slice_idx])}")
print(f"Ground truth unique values: {np.unique(ground_truth[:, :, slice_idx])}")
print(f"Predicted mask unique values: {np.unique(predicted_mask[:, :, slice_idx])}")

# Ensure there is actual data in the selected slice
assert np.max(image[:, :, slice_idx]) > 0, "Selected slice has no meaningful content."
assert np.max(ground_truth[:, :, slice_idx]) > 0, "Ground truth has no meaningful content."
assert np.max(predicted_mask[:, :, slice_idx]) > 0, "Predicted mask has no meaningful content."


plt.figure(figsize=(18, 6))

# Original Image
plt.subplot(1, 3, 1)
plt.title("Original Image (Normalized)")
plt.imshow(image[:, :, slice_idx], cmap="gray")
plt.colorbar()

# Ground Truth with Overlay
plt.subplot(1, 3, 2)
plt.title("Ground Truth")
plt.imshow(image[:, :, slice_idx], cmap="gray")
plt.imshow(ground_truth[:, :, slice_idx], cmap="jet", alpha=0.5)
plt.colorbar()

# Predicted Mask with Overlay
plt.subplot(1, 3, 3)
plt.title("Predicted Mask")
plt.imshow(image[:, :, slice_idx], cmap="gray")
plt.imshow(predicted_mask[:, :, slice_idx], cmap="jet", alpha=0.5)
plt.colorbar()

plt.tight_layout()
plt.show()

"""Computing DSC scores"""

def dice_coefficient(y_true, y_pred, class_id):
    intersection = np.sum((y_true == class_id) & (y_pred == class_id))
    sum_labels = np.sum(y_true == class_id) + np.sum(y_pred == class_id)
    dice = 2 * intersection / sum_labels if sum_labels > 0 else 0
    return dice

for class_id in [1, 2]:
    dice = dice_coefficient(ground_truth, predicted_mask, class_id)
    print(f"Dice coefficient for class {class_id}: {dice:.4f}")

"""Visualizing Test Predictions"""

# File paths
image_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/imagesTs/quiz_047_0000.nii.gz"
predicted_mask_path = "/content/drive/MyDrive/pancreas_project/nnUNet_results/Dataset001_PancreasCancer/test_output/quiz_047.nii.gz"

# Load the image and predicted mask
image = nib.load(image_path).get_fdata()
predicted_mask = nib.load(predicted_mask_path).get_fdata()

# Normalize image to [0, 1] for display
image_min, image_max = np.min(image), np.max(image)
image = (image - image_min) / (image_max - image_min)

# Select a slice with meaningful content
slice_idx = image.shape[2] // 2  # Middle slice
if np.sum(image[:, :, slice_idx]) == 0:  # If slice is empty, find another slice
    slice_idx = next((i for i in range(image.shape[2]) if np.sum(image[:, :, i]) > 0), slice_idx)

print(f"Selected slice index: {slice_idx}")
print(f"Image slice min, max: {np.min(image[:, :, slice_idx])}, {np.max(image[:, :, slice_idx])}")
print(f"Predicted mask unique values: {np.unique(predicted_mask[:, :, slice_idx])}")

# Ensure there is actual data in the selected slice
assert np.max(image[:, :, slice_idx]) > 0, "Selected slice has no meaningful content."
assert np.max(predicted_mask[:, :, slice_idx]) > 0, "Predicted mask has no meaningful content."

plt.figure(figsize=(12, 6))

# Original Image
plt.subplot(1, 2, 1)
plt.title("Original Image (Normalized)")
plt.imshow(image[:, :, slice_idx], cmap="gray")
plt.colorbar()

# Predicted Mask with Overlay
plt.subplot(1, 2, 2)
plt.title("Predicted Mask")
plt.imshow(image[:, :, slice_idx], cmap="gray")
plt.imshow(predicted_mask[:, :, slice_idx], cmap="jet", alpha=0.5)
plt.colorbar()

plt.tight_layout()
plt.show()

"""Computing Class Distribution"""

def compute_class_distribution(y_pred):
    """
    Compute the distribution of predicted classes.
    This provides insights into the frequency of each class in the predictions.
    """
    unique_classes, counts = np.unique(y_pred, return_counts=True)
    distribution = {class_id: count for class_id, count in zip(unique_classes, counts)}
    return distribution

# Predicted mask (assumes predicted_mask is already loaded)
predicted_mask = nib.load("/content/drive/MyDrive/pancreas_project/nnUNet_results/Dataset001_PancreasCancer/test_output/quiz_047.nii.gz").get_fdata()

# Analyze class distribution in the predicted mask
distribution = compute_class_distribution(predicted_mask)

# Display class distribution
print("Class distribution in the predicted mask:")
for class_id, count in distribution.items():
    print(f"Class {int(class_id)}: {count} pixels")

# Optional: Compute Dice coefficient if pseudo-ground truth is available
def dice_coefficient(y_true, y_pred, class_id):
    intersection = np.sum((y_true == class_id) & (y_pred == class_id))
    sum_labels = np.sum(y_true == class_id) + np.sum(y_pred == class_id)
    dice = 2 * intersection / sum_labels if sum_labels > 0 else 0
    return dice

"""CLASSIFICATION
1. FEATURE EXTRACTION OF TRAINING IMAGES

"""

import os, nibabel as nib, numpy as np, json

labels_tr_dir = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/labelsTr"
subtype_mapping_path = os.path.join(
    "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer",
    "subtype_mapping.json"
)

mapping = {}
for label_file in os.listdir(labels_tr_dir):
    if label_file.endswith(".nii.gz"):
        case_id = label_file.replace(".nii.gz", "")
        label = nib.load(os.path.join(labels_tr_dir, label_file)).get_fdata()
        has_lesion = int(np.sum(label == 2) > 0)  # 1 if lesion present, else 0

        mapping[case_id] = {"split": "train", "subtype": has_lesion}

with open(subtype_mapping_path, "w") as f:
    json.dump({"mapping": mapping}, f, indent=4)

print(f"‚úÖ subtype_mapping.json regenerated with {len(mapping)} cases")

import os
import json
import glob
import pathlib
import re

base_path = '/content/drive/MyDrive/pancreas_project'
nnunet_raw_data_path = os.path.join(base_path, 'nnUNet_raw', 'Dataset001_PancreasCancer')
subtype_mapping_path = os.path.join(nnunet_raw_data_path, 'subtype_mapping.json')

# Ensure the nnUNet_raw directory exists
os.makedirs(nnunet_raw_data_path, exist_ok=True)

subtype_mapping = {}
validation_cases = []

# Regex to extract subtype from folder name (e.g., 'subtype0', 'subtype1', 'subtype2')
subtype_regex = re.compile(r'subtype(\d+)')

# Process training data
train_dir = os.path.join(base_path, 'train')
for subtype_folder in glob.glob(os.path.join(train_dir, 'subtype*')):
    if os.path.isdir(subtype_folder):
        match = subtype_regex.search(os.path.basename(subtype_folder))
        if match:
            subtype = int(match.group(1))
            for image_file in glob.glob(os.path.join(subtype_folder, '*_0000.nii.gz')):
                case_id = pathlib.Path(image_file).stem.replace('_0000', '')
                subtype_mapping[case_id] = {'subtype': subtype, 'split': 'train'}
                print(f"Found training case: {case_id}, subtype: {subtype}") # Debug print

# Process validation data
val_dir = os.path.join(base_path, 'validation')
for subtype_folder in glob.glob(os.path.join(val_dir, 'subtype*')):
     if os.path.isdir(subtype_folder):
        match = subtype_regex.search(os.path.basename(subtype_folder))
        if match:
            subtype = int(match.group(1))
            for image_file in glob.glob(os.path.join(subtype_folder, '*_0000.nii.gz')):
                case_id = pathlib.Path(image_file).stem.replace('_0000', '')
                subtype_mapping[case_id] = {'subtype': subtype, 'split': 'validation'}
                validation_cases.append(case_id)
                print(f"Found validation case: {case_id}, subtype: {subtype}") # Debug print


# Save the updated subtype mapping
try:
    with open(subtype_mapping_path, 'w') as f:
        json.dump({"mapping": subtype_mapping, "validation_cases": sorted(list(set(validation_cases)))}, f, indent=4)
    print(f"‚úÖ Updated subtype_mapping.json saved successfully to {subtype_mapping_path}")
    print(f"Number of cases in updated mapping: {len(subtype_mapping)}")
    print(f"Number of validation cases recorded: {len(validation_cases)}")

except Exception as e:
    print(f"‚ùå An error occurred while saving subtype_mapping.json: {e}")

"""Look into your imagesTr/ and labelsTr/ folders.

Compare what‚Äôs inside with what‚Äôs listed in subtype_mapping.json.

Show you which files are missing.
"""

import json, os

subtype_mapping_path = '/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/subtype_mapping.json'

with open(subtype_mapping_path, 'r') as f:
    subtype_mapping = json.load(f)

print("Total entries:", len(subtype_mapping))
# Show first 5
for i, (case_id, info) in enumerate(subtype_mapping.items()):
    print(case_id, ":", info)
    if i == 4:
        break

"""Feature extraction of Training data"""

import os
import json
import nibabel as nib
import numpy as np
import pandas as pd

# Paths
base_path = "/content/drive/MyDrive/pancreas_project"
nnunet_raw_data_path = os.path.join(base_path, "nnUNet_raw", "Dataset001_PancreasCancer")
images_tr_dir = os.path.join(nnunet_raw_data_path, "imagesTr")
labels_tr_dir = os.path.join(nnunet_raw_data_path, "labelsTr")
subtype_mapping_path = os.path.join(nnunet_raw_data_path, "subtype_mapping.json")

# Load mapping
with open(subtype_mapping_path, 'r') as f:
    subtype_mapping = json.load(f)

mapping = subtype_mapping["mapping"]

# Only training cases (since val=0)
train_cases = [case_id.replace(".nii", "") for case_id, info in mapping.items() if info["split"] == "train"]

print(f"üìä Extracting features from {len(train_cases)} training cases...")

feature_data = []

for case_id in train_cases:
    image_path = os.path.join(images_tr_dir, f"{case_id}_0000.nii.gz")
    label_path = os.path.join(labels_tr_dir, f"{case_id}.nii.gz")

    if not os.path.exists(image_path) or not os.path.exists(label_path):
        print(f"‚ö†Ô∏è Missing files for {case_id}, skipping.")
        continue

    try:
        image = nib.load(image_path).get_fdata()
        label = nib.load(label_path).get_fdata()

        # Ensure label is integer
        label = np.round(label).astype(np.uint8)

        # Simple features
        pancreas_volume = np.sum(label == 1)
        lesion_volume   = np.sum(label == 2)

        mean_intensity_pancreas = np.mean(image[label == 1]) if np.any(label == 1) else 0
        mean_intensity_lesion   = np.mean(image[label == 2]) if np.any(label == 2) else 0

        feature_data.append({
            "case_id": case_id,
            "subtype": mapping[case_id + ".nii"]["subtype"],
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "mean_intensity_lesion": mean_intensity_lesion
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# Save features
features_df = pd.DataFrame(feature_data)
features_csv = os.path.join(base_path, "training_features.csv")
features_df.to_csv(features_csv, index=False)

print(f"\n‚úÖ Feature extraction complete! Saved to {features_csv}")
display(features_df.head())

import os
import json
import nibabel as nib
import numpy as np
import pandas as pd
from skimage.measure import regionprops, label as sklabel
from skimage.feature import graycomatrix, graycoprops

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project"
nnunet_raw_data_path = os.path.join(base_path, "nnUNet_raw", "Dataset001_PancreasCancer")
images_tr_dir = os.path.join(nnunet_raw_data_path, "imagesTr")
labels_tr_dir = os.path.join(nnunet_raw_data_path, "labelsTr")
subtype_mapping_path = os.path.join(nnunet_raw_data_path, "subtype_mapping.json")

# -------------------------
# Load mapping
# -------------------------
with open(subtype_mapping_path, 'r') as f:
    subtype_mapping = json.load(f)
mapping = subtype_mapping["mapping"]

# Only training cases
train_cases = [case_id.replace(".nii", "") for case_id, info in mapping.items() if info["split"] == "train"]
print(f"üìä Extracting features from {len(train_cases)} training cases...")

feature_data = []

# -------------------------
# Helper: compute GLCM features
# -------------------------
def compute_glcm_features(patch, levels=32):
    try:
        # normalize intensities 0..levels-1
        patch_norm = np.clip(patch, 0, np.percentile(patch, 99))  # robust clipping
        patch_norm = (patch_norm / patch_norm.max() * (levels - 1)).astype(np.uint8)

        glcm = graycomatrix(patch_norm, distances=[1], angles=[0], levels=levels, symmetric=True, normed=True)

        return {
            "glcm_contrast": graycoprops(glcm, 'contrast')[0, 0],
            "glcm_dissimilarity": graycoprops(glcm, 'dissimilarity')[0, 0],
            "glcm_homogeneity": graycoprops(glcm, 'homogeneity')[0, 0],
            "glcm_energy": graycoprops(glcm, 'energy')[0, 0],
            "glcm_correlation": graycoprops(glcm, 'correlation')[0, 0]
        }
    except Exception:
        return {
            "glcm_contrast": 0,
            "glcm_dissimilarity": 0,
            "glcm_homogeneity": 0,
            "glcm_energy": 0,
            "glcm_correlation": 0
        }

# -------------------------
# Loop over training cases
# -------------------------
for case_id in train_cases:
    image_path = os.path.join(images_tr_dir, f"{case_id}_0000.nii.gz")
    label_path = os.path.join(labels_tr_dir, f"{case_id}.nii.gz")

    if not os.path.exists(image_path) or not os.path.exists(label_path):
        print(f"‚ö†Ô∏è Missing files for {case_id}, skipping.")
        continue

    try:
        image = nib.load(image_path).get_fdata()
        label = nib.load(label_path).get_fdata()

        label = np.round(label).astype(np.uint8)  # Ensure integer labels

        # Volumes
        pancreas_volume = np.sum(label == 1)
        lesion_volume   = np.sum(label == 2)

        # Intensities
        pancreas_voxels = image[label == 1]
        lesion_voxels   = image[label == 2]

        mean_intensity_pancreas = np.mean(pancreas_voxels) if pancreas_voxels.size else 0
        mean_intensity_lesion   = np.mean(lesion_voxels) if lesion_voxels.size else 0
        std_intensity_pancreas  = np.std(pancreas_voxels) if pancreas_voxels.size else 0
        std_intensity_lesion    = np.std(lesion_voxels) if lesion_voxels.size else 0
        p90_lesion              = np.percentile(lesion_voxels, 90) if lesion_voxels.size else 0

        # Ratios
        lesion_to_pancreas_ratio = lesion_volume / (pancreas_volume + 1e-6)

        # Shape + centroid
        lesion_count, max_diameter, lesion_centroid = 0, 0, (0, 0, 0)
        if np.any(label == 2):
            cc = sklabel(label == 2)
            props = regionprops(cc)
            lesion_count = len(props)
            if props:
                max_diameter = max([max(p.major_axis_length, p.minor_axis_length) for p in props])
                lesion_centroid = props[0].centroid  # take first lesion (or average)

        pancreas_size = image.shape
        lesion_cx = lesion_centroid[0] / pancreas_size[0] if pancreas_size[0] else 0
        lesion_cy = lesion_centroid[1] / pancreas_size[1] if pancreas_size[1] else 0
        lesion_cz = lesion_centroid[2] / pancreas_size[2] if pancreas_size[2] else 0

        # Texture (GLCM) from lesion patch
        glcm_feats = compute_glcm_features(lesion_voxels) if lesion_voxels.size else compute_glcm_features(pancreas_voxels)

        # Save all features
        feature_data.append({
            "case_id": case_id,
            "subtype": mapping[case_id + ".nii"]["subtype"],

            # Volume features
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "lesion_to_pancreas_ratio": lesion_to_pancreas_ratio,

            # Intensity features
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "mean_intensity_lesion": mean_intensity_lesion,
            "std_intensity_pancreas": std_intensity_pancreas,
            "std_intensity_lesion": std_intensity_lesion,
            "percentile_90_lesion": p90_lesion,

            # Shape features
            "lesion_count": lesion_count,
            "max_lesion_diameter": max_diameter,

            # Location features
            "lesion_centroid_x": lesion_cx,
            "lesion_centroid_y": lesion_cy,
            "lesion_centroid_z": lesion_cz,

            # Texture features
            #**glcm_feats
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# -------------------------
# Save features
# -------------------------
features_df = pd.DataFrame(feature_data)
features_csv = os.path.join(base_path, "training_features.csv")
features_df.to_csv(features_csv, index=False)

print(f"\n‚úÖ Feature extraction complete! Saved to {features_csv}")
display(features_df.head())

import os
import json
import nibabel as nib
import numpy as np
import pandas as pd
from skimage.measure import regionprops, label as sklabel
from skimage.feature import graycomatrix, graycoprops
from scipy.stats import skew, kurtosis
from scipy.stats import entropy as scipy_entropy

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project"
nnunet_raw_data_path = os.path.join(base_path, "nnUNet_raw", "Dataset001_PancreasCancer")
images_tr_dir = os.path.join(nnunet_raw_data_path, "imagesTr")
labels_tr_dir = os.path.join(nnunet_raw_data_path, "labelsTr")
subtype_mapping_path = os.path.join(nnunet_raw_data_path, "subtype_mapping.json")

# -------------------------
# Load mapping
# -------------------------
with open(subtype_mapping_path, 'r') as f:
    subtype_mapping = json.load(f)
mapping = subtype_mapping["mapping"]

# Only training cases
train_cases = [case_id.replace(".nii", "") for case_id, info in mapping.items() if info["split"] == "train"]
print(f"üìä Extracting features from {len(train_cases)} training cases...")

feature_data = []

# -------------------------
# Helper: compute histogram features
# -------------------------
def compute_histogram_features(voxels):
    if voxels.size == 0:
        return {
            "skewness": 0,
            "kurtosis": 0,
            "entropy": 0
        }
    # Skewness and kurtosis
    sk = skew(voxels, bias=False)
    ku = kurtosis(voxels, bias=False)

    # Entropy (based on histogram probabilities)
    hist, bin_edges = np.histogram(voxels, bins=64, density=True)
    hist = hist + 1e-8  # avoid log(0)
    ent = scipy_entropy(hist)

    return {
        "skewness": sk,
        "kurtosis": ku,
        "entropy": ent
    }

# -------------------------
# Loop over training cases
# -------------------------
for case_id in train_cases:
    image_path = os.path.join(images_tr_dir, f"{case_id}_0000.nii.gz")
    label_path = os.path.join(labels_tr_dir, f"{case_id}.nii.gz")

    if not os.path.exists(image_path) or not os.path.exists(label_path):
        print(f"‚ö†Ô∏è Missing files for {case_id}, skipping.")
        continue

    try:
        image = nib.load(image_path).get_fdata()
        label = nib.load(label_path).get_fdata()

        label = np.round(label).astype(np.uint8)  # Ensure integer labels

        # Volumes
        pancreas_volume = np.sum(label == 1)
        lesion_volume   = np.sum(label == 2)

        # Intensities
        pancreas_voxels = image[label == 1]
        lesion_voxels   = image[label == 2]

        mean_intensity_pancreas = np.mean(pancreas_voxels) if pancreas_voxels.size else 0
        mean_intensity_lesion   = np.mean(lesion_voxels) if lesion_voxels.size else 0
        std_intensity_pancreas  = np.std(pancreas_voxels) if pancreas_voxels.size else 0
        std_intensity_lesion    = np.std(lesion_voxels) if lesion_voxels.size else 0
        p90_lesion              = np.percentile(lesion_voxels, 90) if lesion_voxels.size else 0

        # Ratios
        lesion_to_pancreas_ratio = lesion_volume / (pancreas_volume + 1e-6)

        # Shape + centroid
        lesion_count, max_diameter, lesion_centroid = 0, 0, (0, 0, 0)
        if np.any(label == 2):
            cc = sklabel(label == 2)
            props = regionprops(cc)
            lesion_count = len(props)
            if props:
                max_diameter = max([max(p.major_axis_length, p.minor_axis_length) for p in props])
                lesion_centroid = props[0].centroid  # take first lesion (or average)

        pancreas_size = image.shape
        lesion_cx = lesion_centroid[0] / pancreas_size[0] if pancreas_size[0] else 0
        lesion_cy = lesion_centroid[1] / pancreas_size[1] if pancreas_size[1] else 0
        lesion_cz = lesion_centroid[2] / pancreas_size[2] if pancreas_size[2] else 0

        # Histogram features
        pancreas_hist = compute_histogram_features(pancreas_voxels)
        lesion_hist   = compute_histogram_features(lesion_voxels)

        # Save all features
        feature_data.append({
            "case_id": case_id,
            "subtype": mapping[case_id + ".nii"]["subtype"],

            # Volume features
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "lesion_to_pancreas_ratio": lesion_to_pancreas_ratio,

            # Intensity features
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "mean_intensity_lesion": mean_intensity_lesion,
            "std_intensity_pancreas": std_intensity_pancreas,
            "std_intensity_lesion": std_intensity_lesion,
            "percentile_90_lesion": p90_lesion,

            # Histogram features
            "pancreas_skewness": pancreas_hist["skewness"],
            "pancreas_kurtosis": pancreas_hist["kurtosis"],
            "pancreas_entropy": pancreas_hist["entropy"],

            "lesion_skewness": lesion_hist["skewness"],
            "lesion_kurtosis": lesion_hist["kurtosis"],
            "lesion_entropy": lesion_hist["entropy"],

            # Shape features
            "lesion_count": lesion_count,
            "max_lesion_diameter": max_diameter,

            # Location features
            "lesion_centroid_x": lesion_cx,
            "lesion_centroid_y": lesion_cy,
            "lesion_centroid_z": lesion_cz
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# -------------------------
# Save features
# -------------------------
features_df = pd.DataFrame(feature_data)
features_csv = os.path.join(base_path, "training_features.csv")
features_df.to_csv(features_csv, index=False)

print(f"\n‚úÖ Feature extraction complete! Saved to {features_csv}")
display(features_df.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from imblearn.over_sampling import SMOTE

# -------------------------
# Load features
# -------------------------
features_csv = "/content/drive/MyDrive/pancreas_project/training_features_safe.csv"
df = pd.read_csv(features_csv)

X = df.drop(columns=["case_id", "subtype"])
y = df["subtype"]

# -------------------------
# SMOTE for balancing
# -------------------------
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print(f"Original class distribution:\n{y.value_counts(normalize=True)}\n")
print(f"Balanced class distribution:\n{pd.Series(y_res).value_counts(normalize=True)}\n")

# -------------------------
# Stratified K-Fold
# -------------------------
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
all_reports = []
feature_importances = np.zeros(X.shape[1])
conf_matrices = []

for fold, (train_idx, test_idx) in enumerate(skf.split(X_res, y_res), 1):
    X_train, X_test = X_res.iloc[train_idx], X_res.iloc[test_idx]
    y_train, y_test = y_res[train_idx], y_res[test_idx]

    # RandomForest with basic hyperparameter tuning
    param_grid = {
        "n_estimators": [200],
        "max_depth": [None, 10],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2]
    }

    rf = RandomForestClassifier(random_state=42)
    grid = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train, y_train)
    best_rf = grid.best_estimator_

    # Predictions
    y_pred = best_rf.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    all_reports.append(report)
    conf_matrices.append(confusion_matrix(y_test, y_pred))

    feature_importances += best_rf.feature_importances_

    print(f"\nüìå Fold {fold} accuracy: {accuracy_score(y_test, y_pred):.4f}")

# -------------------------
# Average feature importance
# -------------------------
feature_importances /= skf.get_n_splits()
feature_importance_df = pd.DataFrame({
    "feature": X.columns,
    "importance": feature_importances
}).sort_values(by="importance", ascending=False)

# -------------------------
# Average classification report
# -------------------------
def average_report(reports, classes):
    avg = {}
    for cls in classes:
        avg[cls] = {}
        for metric in ["precision", "recall", "f1-score"]:
            avg[cls][metric] = np.mean([r[str(cls)][metric] for r in reports])
    avg["accuracy"] = np.mean([r["accuracy"] for r in reports])
    return avg

avg_report = average_report(all_reports, sorted(y.unique()))
print("\n‚úÖ Average Classification Report (5-fold CV):")
for cls, metrics in avg_report.items():
    print(f"{cls}: {metrics}")

print("\nüìå Top 10 Features by Importance:")
print(feature_importance_df.head(10))

# -------------------------
# Plot feature importance
# -------------------------
plt.figure(figsize=(10,6))
sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15))
plt.title("Top 15 Feature Importances")
plt.tight_layout()
plt.show()

# -------------------------
# Plot average confusion matrix
# -------------------------
avg_conf_matrix = np.mean(np.array(conf_matrices), axis=0)
plt.figure(figsize=(6,5))
sns.heatmap(avg_conf_matrix, annot=True, fmt=".1f", cmap="Blues",
            xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Average Confusion Matrix (5-fold CV)")
plt.show()

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE

# -------------------------
# Load extracted features
# -------------------------
features_csv = "/content/drive/MyDrive/pancreas_project/training_features.csv"
df = pd.read_csv(features_csv)

# -------------------------
# Feature Engineering
# -------------------------
df['lesion_to_pancreas_ratio'] = df['lesion_volume'] / (df['pancreas_volume'] + 1e-6)
df['intensity_ratio'] = df['mean_intensity_lesion'] / (df['mean_intensity_pancreas'] + 1e-6)
df['std_ratio'] = df['std_intensity_lesion'] / (df['std_intensity_pancreas'] + 1e-6)
df['volume_intensity_interaction'] = df['lesion_volume'] * df['mean_intensity_lesion']
df['lesion_range'] = df['percentile_90_lesion'] - df['mean_intensity_lesion']
df['pancreas_cv'] = df['std_intensity_pancreas'] / (df['mean_intensity_pancreas'] + 1e-6)
df['lesion_cv'] = df['std_intensity_lesion'] / (df['mean_intensity_lesion'] + 1e-6)
df['lesion_diameter_ratio'] = df['max_lesion_diameter'] / (df['pancreas_volume'] + 1e-6)
df['centroid_distance'] = np.sqrt(df['lesion_centroid_x']**2 + df['lesion_centroid_y']**2 + df['lesion_centroid_z']**2)

# -------------------------
# Prepare data
# -------------------------
X = df.drop(columns=["case_id", "subtype"])
y = df["subtype"]

# -------------------------
# Balance classes using SMOTE
# -------------------------
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print(f"Original class distribution:\n{y.value_counts(normalize=True)}\n")
print(f"Balanced class distribution:\n{pd.Series(y_res).value_counts(normalize=True)}\n")

# -------------------------
# Stratified K-Fold CV
# -------------------------
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
all_reports = []
feature_importances = np.zeros(X.shape[1])

for fold, (train_idx, test_idx) in enumerate(skf.split(X_res, y_res), 1):
    X_train, X_test = X_res.iloc[train_idx], X_res.iloc[test_idx]
    y_train, y_test = y_res[train_idx], y_res[test_idx]

    # RandomForest + basic hyperparameter tuning
    param_grid = {
        "n_estimators": [200, 300],
        "max_depth": [None, 5, 10],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2]
    }

    rf = RandomForestClassifier(random_state=42)
    grid = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train, y_train)
    best_rf = grid.best_estimator_

    # Predictions
    y_pred = best_rf.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    all_reports.append(report)

    # Aggregate feature importances
    feature_importances += best_rf.feature_importances_

    print(f"\nüìå Fold {fold} accuracy: {accuracy_score(y_test, y_pred):.4f}")

# -------------------------
# Average feature importance
# -------------------------
feature_importances /= skf.get_n_splits()
feature_importance_df = pd.DataFrame({
    "feature": X.columns,
    "importance": feature_importances
}).sort_values(by="importance", ascending=False)

# -------------------------
# Average classification report
# -------------------------
def average_report(reports, classes):
    avg = {}
    for cls in classes:
        avg[cls] = {}
        for metric in ["precision", "recall", "f1-score"]:
            avg[cls][metric] = np.mean([r[cls][metric] for r in reports])
    avg["accuracy"] = np.mean([r["accuracy"] for r in reports])
    return avg

avg_report = average_report(all_reports, [str(c) for c in sorted(y.unique())])

print("\n‚úÖ Average Classification Report (5-fold CV):")
for cls, metrics in avg_report.items():
    print(f"{cls}: {metrics}")

print("\nüìå Top 10 Features by Importance:")
print(feature_importance_df.head(10))

import pandas as pd
from sklearn.utils import shuffle

# Load your existing features
features_csv = "/content/drive/MyDrive/pancreas_project/training_features.csv"
df = pd.read_csv(features_csv)

# Duplicate the dataset (e.g., double it)
df_expanded = pd.concat([df, df], axis=0)  # duplicate once, can repeat more if needed
df_expanded = shuffle(df_expanded, random_state=42).reset_index(drop=True)

print(f"Original dataset size: {df.shape[0]}")
print(f"Expanded dataset size: {df_expanded.shape[0]}")

# Save the expanded dataset (optional)
expanded_csv = "/content/drive/MyDrive/pancreas_project/training_features_expanded.csv"
df_expanded.to_csv(expanded_csv, index=False)
print(f"Expanded dataset saved to: {expanded_csv}")

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import PolynomialFeatures

# -------------------------
# Load expanded dataset
# -------------------------
features_csv = "/content/drive/MyDrive/pancreas_project/training_features_expanded.csv"
df = pd.read_csv(features_csv)

# -------------------------
# Feature Engineering
# -------------------------
df['lesion_to_pancreas_ratio'] = df['lesion_volume'] / (df['pancreas_volume'] + 1e-6)
df['intensity_ratio'] = df['mean_intensity_lesion'] / (df['mean_intensity_pancreas'] + 1e-6)
df['std_ratio'] = df['std_intensity_lesion'] / (df['std_intensity_pancreas'] + 1e-6)
df['volume_intensity_interaction'] = df['lesion_volume'] * df['mean_intensity_lesion']
df['lesion_range'] = df['percentile_90_lesion'] - df['mean_intensity_lesion']
df['pancreas_cv'] = df['std_intensity_pancreas'] / (df['mean_intensity_pancreas'] + 1e-6)
df['lesion_cv'] = df['std_intensity_lesion'] / (df['mean_intensity_lesion'] + 1e-6)
df['lesion_diameter_ratio'] = df['max_lesion_diameter'] / (df['pancreas_volume'] + 1e-6)
df['centroid_distance'] = np.sqrt(df['lesion_centroid_x']**2 + df['lesion_centroid_y']**2 + df['lesion_centroid_z']**2)

# -------------------------
# Polynomial / interaction features
# -------------------------
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
poly_features = poly.fit_transform(df[['lesion_volume', 'pancreas_skewness', 'lesion_to_pancreas_ratio']])
poly_feature_names = poly.get_feature_names_out(['lesion_volume', 'pancreas_skewness', 'lesion_to_pancreas_ratio'])
df_poly = pd.DataFrame(poly_features, columns=poly_feature_names)
df = pd.concat([df.reset_index(drop=True), df_poly.reset_index(drop=True)], axis=1)

# -------------------------
# Prepare data
# -------------------------
X = df.drop(columns=["case_id", "subtype"])
y = df["subtype"]

# -------------------------
# Balance classes using SMOTE
# -------------------------
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print(f"Original class distribution:\n{y.value_counts(normalize=True)}\n")
print(f"Balanced class distribution:\n{pd.Series(y_res).value_counts(normalize=True)}\n")

# -------------------------
# Stratified K-Fold CV
# -------------------------
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
all_reports = []
feature_importances = np.zeros(X_res.shape[1])

for fold, (train_idx, test_idx) in enumerate(skf.split(X_res, y_res), 1):
    X_train, X_test = X_res.iloc[train_idx], X_res.iloc[test_idx]
    y_train, y_test = y_res[train_idx], y_res[test_idx]

    # RandomForest with basic hyperparameter tuning
    param_grid = {
        "n_estimators": [200, 300],
        "max_depth": [None, 5, 10],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2]
    }

    rf = RandomForestClassifier(random_state=42)
    grid = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train, y_train)
    best_rf = grid.best_estimator_

    # Predictions
    y_pred = best_rf.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    all_reports.append(report)

    # Aggregate feature importances
    feature_importances += best_rf.feature_importances_

    print(f"\nüìå Fold {fold} accuracy: {accuracy_score(y_test, y_pred):.4f}")

# -------------------------
# Average feature importance
# -------------------------
feature_importances /= skf.get_n_splits()
feature_importance_df = pd.DataFrame({
    "feature": X_res.columns,
    "importance": feature_importances
}).sort_values(by="importance", ascending=False)

# -------------------------
# Average classification report
# -------------------------
def average_report(reports, classes):
    avg = {}
    for cls in classes:
        avg[cls] = {}
        for metric in ["precision", "recall", "f1-score"]:
            avg[cls][metric] = np.mean([r[cls][metric] for r in reports])
    avg["accuracy"] = np.mean([r["accuracy"] for r in reports])
    return avg

avg_report = average_report(all_reports, [str(c) for c in sorted(y.unique())])

print("\n‚úÖ Average Classification Report (5-fold CV):")
for cls, metrics in avg_report.items():
    print(f"{cls}: {metrics}")

print("\nüìå Top 10 Features by Importance:")
print(feature_importance_df.head(10))
training_feature_names = X.columns.tolist()

import pickle

# -------------------------
# Save training feature names
# -------------------------
training_feature_names = X_train.columns.tolist()  # After all feature engineering
with open("/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/training_feature_names.pkl", "wb") as f:
    pickle.dump(training_feature_names, f)

# -------------------------
# Save polynomial transformer
# -------------------------
with open("/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/poly_transformer.pkl", "wb") as f:
    pickle.dump(poly, f)  # `poly` is the PolynomialFeatures object you fitted on training data

# -------------------------
# Save trained Random Forest model
# -------------------------
with open("/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/best_rf_model.pkl", "wb") as f:
    pickle.dump(best_rf, f)

print("‚úÖ Training artifacts saved: training_feature_names.pkl, poly_transformer.pkl, best_rf_model.pkl")

"""Feature Extraction of Validation Data"""

import os
import nibabel as nib
import numpy as np
import pandas as pd
import json
from skimage.measure import regionprops, label as sklabel
from scipy.stats import skew, kurtosis
from scipy.stats import entropy as scipy_entropy
from sklearn.preprocessing import PolynomialFeatures

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer"
# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer"
images_val_dir = os.path.join(base_path, "imagesVal")
labels_val_dir = os.path.join(base_path, "labelsVal")
subtype_mapping_path = os.path.join(base_path, "subtype_mapping.json")

# -------------------------
# Load mapping
# -------------------------
with open(subtype_mapping_path, 'r') as f:
    subtype_mapping = json.load(f)
mapping = subtype_mapping["mapping"]

# -------------------------
# Validation cases (split == 'validation')
# -------------------------
val_cases = [case_id for case_id, info in mapping.items() if info["split"] == "validation"]
print(f"üìä Extracting features from {len(val_cases)} validation cases...")

feature_data = []

# -------------------------
# Helper: compute histogram features
# -------------------------
def compute_histogram_features(voxels):
    if voxels.size == 0:
        return {
            "skewness": 0,
            "kurtosis": 0,
            "entropy": 0
        }
    # Skewness and kurtosis
    sk = skew(voxels, bias=False)
    ku = kurtosis(voxels, bias=False)

    # Entropy (based on histogram probabilities)
    hist, bin_edges = np.histogram(voxels, bins=64, density=True)
    hist = hist + 1e-8  # avoid log(0)
    ent = scipy_entropy(hist)

    return {
        "skewness": sk,
        "kurtosis": ku,
        "entropy": ent
    }


# -------------------------
# Loop over validation cases
# -------------------------
for case_id in val_cases:
    image_id = case_id.replace(".nii", "")  # remove .nii for image path
    image_path = os.path.join(images_val_dir, f"{image_id}_0000.nii.gz")
    label_path = os.path.join(labels_val_dir, f"{image_id}.nii.gz")

    if not os.path.exists(image_path) or not os.path.exists(label_path):
        print(f"‚ö†Ô∏è Missing files for {case_id}, skipping.")
        continue

    try:
        image = nib.load(image_path)
        label = nib.load(label_path)
        label_data = label.get_fdata().astype(np.uint8)

        pancreas_mask = (label_data == 1)
        lesion_mask = (label_data == 2)

        # Basic features
        pancreas_volume = np.sum(pancreas_mask)
        lesion_volume = np.sum(lesion_mask)

        # Intensities
        pancreas_voxels = image.get_fdata()[pancreas_mask]
        lesion_voxels   = image.get_fdata()[lesion_mask]

        mean_intensity_pancreas = np.mean(pancreas_voxels) if pancreas_voxels.size else 0
        mean_intensity_lesion   = np.mean(lesion_voxels) if lesion_voxels.size else 0
        std_intensity_pancreas = np.std(pancreas_voxels) if pancreas_voxels.size else 0
        std_intensity_lesion = np.std(lesion_voxels) if lesion_voxels.size else 0
        p90_lesion              = np.percentile(lesion_voxels, 90) if lesion_voxels.size else 0

        # Histogram features
        pancreas_hist = compute_histogram_features(pancreas_voxels)
        lesion_hist   = compute_histogram_features(lesion_voxels)

        # Shape + centroid
        lesion_count, max_diameter, lesion_centroid = 0, 0, (0, 0, 0)
        if np.any(label_data == 2):
            cc = sklabel(label_data == 2)
            props = regionprops(cc)
            lesion_count = len(props)
            if props:
                max_diameter = max([max(p.major_axis_length, p.minor_axis_length) for p in props])
                lesion_centroid = props[0].centroid  # take first lesion (or average)

        pancreas_size = image.get_fdata().shape
        lesion_cx = lesion_centroid[0] / pancreas_size[0] if pancreas_size[0] else 0
        lesion_cy = lesion_centroid[1] / pancreas_size[1] if pancreas_size[1] else 0
        lesion_cz = lesion_centroid[2] / pancreas_size[2] if pancreas_size[2] else 0

        feature_data.append({
            "case_id": case_id,
            "subtype": mapping[case_id]["subtype"],
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "mean_intensity_lesion": mean_intensity_lesion,
            "std_intensity_pancreas": std_intensity_pancreas,
            "std_intensity_lesion": std_intensity_lesion,
            "percentile_90_lesion": p90_lesion,
            "pancreas_skewness": pancreas_hist["skewness"],
            "pancreas_kurtosis": pancreas_hist["kurtosis"],
            "pancreas_entropy": pancreas_hist["entropy"],
            "lesion_skewness": lesion_hist["skewness"],
            "lesion_kurtosis": lesion_hist["kurtosis"],
            "lesion_entropy": lesion_hist["entropy"],
            "lesion_count": lesion_count,
            "max_lesion_diameter": max_diameter,
            "lesion_centroid_x": lesion_cx,
            "lesion_centroid_y": lesion_cy,
            "lesion_centroid_z": lesion_cz
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# -------------------------
# Create DataFrame and Feature Engineering
# -------------------------
features_df = pd.DataFrame(feature_data)

# Derived features (match training)
features_df['lesion_to_pancreas_ratio'] = features_df['lesion_volume'] / (features_df['pancreas_volume'] + 1e-6)
features_df['intensity_ratio'] = features_df['mean_intensity_lesion'] / (features_df['mean_intensity_pancreas'] + 1e-6)
features_df['std_ratio'] = features_df['std_intensity_lesion'] / (features_df['std_intensity_pancreas'] + 1e-6)
features_df['volume_intensity_interaction'] = features_df['lesion_volume'] * features_df['mean_intensity_lesion']
features_df['lesion_range'] = features_df['percentile_90_lesion'] - features_df['mean_intensity_lesion']
features_df['pancreas_cv'] = features_df['std_intensity_pancreas'] / (features_df['mean_intensity_pancreas'] + 1e-6)
features_df['lesion_cv'] = features_df['std_intensity_lesion'] / (features_df['mean_intensity_lesion'] + 1e-6)
features_df['lesion_diameter_ratio'] = features_df['max_lesion_diameter'] / (features_df['pancreas_volume'] + 1e-6)
features_df['centroid_distance'] = np.sqrt(features_df['lesion_centroid_x']**2 + features_df['lesion_centroid_y']**2 + features_df['lesion_centroid_z']**2)

# Polynomial / interaction features (match training)
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
# Ensure the columns exist and handle potential missing values or NaNs
cols_for_poly = ['lesion_volume', 'pancreas_skewness', 'lesion_to_pancreas_ratio']
for col in cols_for_poly:
    if col not in features_df.columns:
        features_df[col] = 0 # Or some other default value

# Handle NaNs or infinite values before applying PolynomialFeatures
features_df[cols_for_poly] = features_df[cols_for_poly].replace([np.inf, -np.inf], np.nan).fillna(0)


poly_features = poly.fit_transform(features_df[cols_for_poly])
poly_feature_names = poly.get_feature_names_out(cols_for_poly)
df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=features_df.index)
features_df = pd.concat([features_df.reset_index(drop=True), df_poly.reset_index(drop=True)], axis=1)


# -------------------------
# Save validation features
# -------------------------
validation_csv = os.path.join("/content/drive/MyDrive/pancreas_project/validation_features.csv")
features_df.to_csv(validation_csv, index=False)

print(f"\n‚úÖ Validation feature extraction complete! Saved to {validation_csv}")
display(features_df.head())

import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import PolynomialFeatures # Import PolynomialFeatures

# -------------------------
# Paths
# -------------------------
validation_csv = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/validation_features.csv"

# -------------------------
# Load validation features
# -------------------------
val_df = pd.read_csv(validation_csv)

# Separate features and labels
X_val = val_df.drop(columns=["case_id", "subtype"])
y_val = val_df["subtype"]

# Ensure the columns of X_val match the columns X_res used for training
# Identify the columns from X_res (from the training cell CZHDuWID_dDz)
# Note: This requires manually inspecting the output of the training cell to get the exact column names.
# For a more robust solution, save the feature names during training.

# Assuming the feature names from training are available in a list called `training_feature_names`
# For now, we will assume X_val already has the correct columns due to the fix in mq767gwjN5xV

# -------------------------
# Use trained Random Forest (from your previous training)
# -------------------------
# Assuming `best_rf` is the trained RF model from training step

# Predict on validation set
y_pred_val = best_rf.predict(X_val)

# -------------------------
# Evaluation
# -------------------------
print("üìå Validation Accuracy:", accuracy_score(y_val, y_pred_val))
print("\n‚úÖ Classification Report:")
print(classification_report(y_val, y_pred_val))

# -------------------------
# Optional: Add predictions to the dataframe
# -------------------------
val_df["predicted_subtype"] = y_pred_val
val_df.to_csv(validation_csv.replace(".csv", "_with_predictions.csv"), index=False)
print(f"\nüìå Validation predictions saved to {validation_csv.replace('.csv', '_with_predictions.csv')}")

# ================================
# Validation Prediction Pipeline
# ================================
import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import pickle

# -------------------------
# Paths
# -------------------------
validation_csv = "/content/drive/MyDrive/pancreas_project/validation_features.csv"
training_feature_names_path = "/content/drive/MyDrive/pancreas_project/training_feature_names.pkl"
trained_rf_model_path = "/content/drive/MyDrive/pancreas_project/best_rf_model.pkl"

# -------------------------
# Load validation features
# -------------------------
val_df = pd.read_csv(validation_csv)

# Separate features and labels
X_val = val_df.drop(columns=["case_id", "subtype"])
y_val = val_df["subtype"]

# -------------------------
# Remove duplicate columns if any
# -------------------------
X_val = X_val.loc[:, ~X_val.columns.duplicated()]

# -------------------------
# Feature Engineering (same as training)
# -------------------------
X_val['lesion_to_pancreas_ratio'] = X_val['lesion_volume'] / (X_val['pancreas_volume'] + 1e-6)
X_val['intensity_ratio'] = X_val['mean_intensity_lesion'] / (X_val['mean_intensity_pancreas'] + 1e-6)
X_val['std_ratio'] = X_val['std_intensity_lesion'] / (X_val['std_intensity_pancreas'] + 1e-6)
X_val['volume_intensity_interaction'] = X_val['lesion_volume'] * X_val['mean_intensity_lesion']
X_val['lesion_range'] = np.percentile(X_val['mean_intensity_lesion'], 90) - X_val['mean_intensity_lesion']
X_val['pancreas_cv'] = X_val['std_intensity_pancreas'] / (X_val['mean_intensity_pancreas'] + 1e-6)
X_val['lesion_cv'] = X_val['std_intensity_lesion'] / (X_val['mean_intensity_lesion'] + 1e-6)
X_val['lesion_diameter_ratio'] = X_val['max_lesion_diameter'] / (X_val['pancreas_volume'] + 1e-6)
X_val['centroid_distance'] = np.sqrt(
    X_val['lesion_centroid_x']**2 +
    X_val['lesion_centroid_y']**2 +
    X_val['lesion_centroid_z']**2
)

# -------------------------
# Load training feature names
# -------------------------
with open(training_feature_names_path, "rb") as f:
    training_feature_names = pickle.load(f)

# Reindex validation features to match training
X_val = X_val.reindex(columns=training_feature_names, fill_value=0)

# -------------------------
# Load trained Random Forest
# -------------------------
with open(trained_rf_model_path, "rb") as f:
    best_rf = pickle.load(f)

# -------------------------
# Predict on validation set
# -------------------------
y_pred_val = best_rf.predict(X_val)

# -------------------------
# Evaluation
# -------------------------
print("üìå Validation Accuracy:", accuracy_score(y_val, y_pred_val))
print("\n‚úÖ Classification Report:")
print(classification_report(y_val, y_pred_val))

# -------------------------
# Save predictions
# -------------------------
val_df["predicted_subtype"] = y_pred_val
pred_csv_path = validation_csv.replace(".csv", "_with_predictions.csv")
val_df.to_csv(pred_csv_path, index=False)
print(f"\nüìå Validation predictions saved to {pred_csv_path}")

import os
import nibabel as nib
import numpy as np
import pandas as pd

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer"
images_test_dir = os.path.join(base_path, "imagesTs")  # test images folder

# -------------------------
# Test cases
# -------------------------
test_cases = [f for f in os.listdir(images_test_dir) if f.endswith("_0000.nii.gz")]
print(f"üìä Extracting features from {len(test_cases)} test cases...")

feature_data = []

# -------------------------
# Loop over test cases
# -------------------------
for file_name in test_cases:
    case_id = file_name.replace("_0000.nii.gz", "")
    image_path = os.path.join(images_test_dir, file_name)

    try:
        image = nib.load(image_path)
        image_data = image.get_fdata()

        # Basic features
        feature_data.append({
            "case_id": case_id,
            "mean_intensity": np.mean(image_data),
            "std_intensity": np.std(image_data),
            "max_intensity": np.max(image_data),
            "min_intensity": np.min(image_data)
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# -------------------------
# Save test features
# -------------------------
features_df = pd.DataFrame(feature_data)
test_csv = os.path.join(base_path, "test_features.csv")
features_df.to_csv(test_csv, index=False)

print(f"\n‚úÖ Test feature extraction complete! Saved to {test_csv}")
features_df.head()

!pip install nnUNetv2

!pip install nnUNetv2

!pip uninstall -y nnunetv2

# Commented out IPython magic to ensure Python compatibility.
# Clone the nnU-Net v2 repo (if not already)
!git clone https://github.com/MIC-DKFZ/nnUNet.git
# %cd nnUNet

!pip install -e /content/nnUNet

import os
os.environ['nnUNet_raw_data_base'] = "/content/drive/MyDrive/pancreas_project/nnUNet_raw"
os.environ['nnUNet_preprocessed'] = "/content/drive/MyDrive/pancreas_project/nnUNet_preprocessed"
os.environ['RESULTS_FOLDER'] = "/content/drive/MyDrive/pancreas_project/nnUNet_trained_models"

!nnUNet_print_available_pretrained_models

import os
import nibabel as nib
import numpy as np
import pandas as pd

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer"
images_test_dir = os.path.join(base_path, "imagesTs")
test_features_csv = os.path.join(base_path, "test_features.csv")
pred_masks_dir = os.path.join(base_path, "predicted_masks")
os.makedirs(pred_masks_dir, exist_ok=True)

# -------------------------
# Placeholder for model prediction
# -------------------------
def predict_mask(image_data):
    """
    Replace this with your actual segmentation model inference.
    Currently returns a dummy mask of zeros.
    """
    # Example: create a fake mask for demonstration
    mask = np.zeros_like(image_data, dtype=np.uint8)
    # For demonstration, let's assume voxels with intensity > 50 are lesions
    mask[image_data > 50] = 1  # label 1 = pancreas/lesion
    return mask

# -------------------------
# Loop over test images
# -------------------------
feature_data = []
test_cases = [f for f in os.listdir(images_test_dir) if f.endswith(".nii.gz")]

for img_file in test_cases:
    case_id = img_file.replace("_0000.nii.gz", "")
    image_path = os.path.join(images_test_dir, img_file)

    try:
        # Load image
        img_nib = nib.load(image_path)
        img_data = img_nib.get_fdata()

        # Predict mask
        pred_mask = predict_mask(img_data)

        # Save predicted mask
        mask_path = os.path.join(pred_masks_dir, f"{case_id}_pred_mask.nii.gz")
        nib.save(nib.Nifti1Image(pred_mask.astype(np.uint8), img_nib.affine), mask_path)

        # Extract features (same as training/validation)
        pancreas_mask = (pred_mask == 1)
        lesion_mask = (pred_mask == 2)  # if you have multiple labels

        pancreas_volume = np.sum(pancreas_mask)
        lesion_volume = np.sum(lesion_mask)
        mean_intensity_pancreas = np.mean(img_data[pancreas_mask]) if np.sum(pancreas_mask) > 0 else 0
        mean_intensity_lesion = np.mean(img_data[lesion_mask]) if np.sum(lesion_mask) > 0 else 0
        std_intensity_pancreas = np.std(img_data[pancreas_mask]) if np.sum(pancreas_mask) > 0 else 0
        std_intensity_lesion = np.std(img_data[lesion_mask]) if np.sum(lesion_mask) > 0 else 0
        lesion_to_pancreas_ratio = lesion_volume / (pancreas_volume + 1e-6)
        intensity_ratio = mean_intensity_lesion / (mean_intensity_pancreas + 1e-6)
        std_ratio = std_intensity_lesion / (std_intensity_pancreas + 1e-6)

        feature_data.append({
            "case_id": case_id,
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "mean_intensity_lesion": mean_intensity_lesion,
            "std_intensity_pancreas": std_intensity_pancreas,
            "std_intensity_lesion": std_intensity_lesion,
            "lesion_to_pancreas_ratio": lesion_to_pancreas_ratio,
            "intensity_ratio": intensity_ratio,
            "std_ratio": std_ratio
        })

        print(f"‚úÖ Processed {case_id}")

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# -------------------------
# Save test features
# -------------------------
test_df = pd.DataFrame(feature_data)
test_df.to_csv(test_features_csv, index=False)
print(f"\nüìå Test features saved to {test_features_csv}")
print(f"üìå Predicted masks saved to {pred_masks_dir}")

# Paths and Imports
import os
import nibabel as nib
import numpy as np
import pandas as pd
import json

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer"
images_test_dir = os.path.join(base_path, "imagesTs")
predicted_masks_dir = os.path.join(base_path, "predicted_masks")  # folder where predicted masks are saved

# Load predicted masks
# Get all predicted mask files
mask_files = [f for f in os.listdir(predicted_masks_dir) if f.endswith(".nii.gz")]
print(f"üìå Found {len(mask_files)} predicted masks.")

#Loop over test cases and calculate features
feature_data = []

for mask_file in mask_files:
    # Remove .nii.gz and the _pred_mask suffix to get the original case_id
    case_id = mask_file.replace(".nii.gz", "").replace("_pred_mask", "")

    # Original image path
    image_path = os.path.join(images_test_dir, f"{case_id}_0000.nii.gz")
    mask_path = os.path.join(predicted_masks_dir, mask_file)

    if not os.path.exists(image_path) or not os.path.exists(mask_path):
        print(f"‚ö†Ô∏è Missing files for {case_id}, skipping.")
        continue

    # --- Continue with feature extraction as before ---


    try:
        image = nib.load(image_path)
        mask = nib.load(mask_path)
        mask_data = mask.get_fdata().astype(np.uint8)

        pancreas_mask = (mask_data == 1)
        lesion_mask = (mask_data == 2)

        # Skip if no lesion
        if np.sum(lesion_mask) == 0:
            print(f"‚ö†Ô∏è No lesion found for {case_id}, skipping.")
            continue

        # -------------------------
        # Feature calculation
        # -------------------------
        pancreas_volume = np.sum(pancreas_mask)
        lesion_volume = np.sum(lesion_mask)
        mean_intensity_pancreas = np.mean(image.get_fdata()[pancreas_mask])
        mean_intensity_lesion = np.mean(image.get_fdata()[lesion_mask])
        std_intensity_pancreas = np.std(image.get_fdata()[pancreas_mask])
        std_intensity_lesion = np.std(image.get_fdata()[lesion_mask])
        max_lesion_diameter = np.ptp(np.argwhere(lesion_mask), axis=0)  # NumPy >=2.0 safe
        lesion_centroid = np.mean(np.argwhere(lesion_mask), axis=0)

        # Derived features
        lesion_to_pancreas_ratio = lesion_volume / (pancreas_volume + 1e-6)
        intensity_ratio = mean_intensity_lesion / (mean_intensity_pancreas + 1e-6)
        std_ratio = std_intensity_lesion / (std_intensity_pancreas + 1e-6)
        volume_intensity_interaction = lesion_volume * mean_intensity_lesion
        lesion_range = np.percentile(image.get_fdata()[lesion_mask], 90) - mean_intensity_lesion
        pancreas_cv = std_intensity_pancreas / (mean_intensity_pancreas + 1e-6)
        lesion_cv = std_intensity_lesion / (mean_intensity_lesion + 1e-6)
        lesion_diameter_ratio = np.max(max_lesion_diameter) / (pancreas_volume + 1e-6)
        centroid_distance = np.linalg.norm(lesion_centroid)

        feature_data.append({
            "case_id": case_id,
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "mean_intensity_lesion": mean_intensity_lesion,
            "std_intensity_pancreas": std_intensity_pancreas,
            "std_intensity_lesion": std_intensity_lesion,
            "max_lesion_diameter": np.max(max_lesion_diameter),
            "lesion_centroid_x": lesion_centroid[0],
            "lesion_centroid_y": lesion_centroid[1],
            "lesion_centroid_z": lesion_centroid[2],
            "lesion_to_pancreas_ratio": lesion_to_pancreas_ratio,
            "intensity_ratio": intensity_ratio,
            "std_ratio": std_ratio,
            "volume_intensity_interaction": volume_intensity_interaction,
            "lesion_range": lesion_range,
            "pancreas_cv": pancreas_cv,
            "lesion_cv": lesion_cv,
            "lesion_diameter_ratio": lesion_diameter_ratio,
            "centroid_distance": centroid_distance
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

import os
import nibabel as nib
import numpy as np
import pandas as pd
import json

# -------------------------
# Paths
# -------------------------
base_path = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer"
images_test_dir = os.path.join(base_path, "imagesTs")
predicted_masks_dir = os.path.join(base_path, "predicted_masks")
test_features_csv = os.path.join(base_path, "test_features.csv")

# -------------------------
# Validation: load mapping if needed
# -------------------------
# For test set, we may not have true subtypes
# If available, can load as mapping; else skip
subtype_mapping_path = os.path.join(base_path, "subtype_mapping.json")
if os.path.exists(subtype_mapping_path):
    with open(subtype_mapping_path, 'r') as f:
        subtype_mapping = json.load(f)
    mapping = subtype_mapping.get("mapping", {})
else:
    mapping = {}

# -------------------------
# Test cases
# -------------------------
test_cases = [f.replace("_pred_mask.nii.gz","") for f in os.listdir(predicted_masks_dir) if f.endswith(".nii.gz")]
print(f"üìä Extracting features from {len(test_cases)} test cases...")

feature_data = []

# -------------------------
# Loop over test cases
# -------------------------
for case_id in test_cases:
    # Original image
    image_path = os.path.join(images_test_dir, f"{case_id}_0000.nii.gz")
    # Predicted mask
    mask_path = os.path.join(predicted_masks_dir, f"{case_id}_pred_mask.nii.gz")

    if not os.path.exists(image_path) or not os.path.exists(mask_path):
        print(f"‚ö†Ô∏è Missing files for {case_id}, skipping.")
        continue

    try:
        image = nib.load(image_path)
        mask = nib.load(mask_path)
        mask_data = mask.get_fdata().astype(np.uint8)

        pancreas_mask = (mask_data > 0)  # treat all mask voxels as pancreas
        lesion_mask = (mask_data == 2)  # if class 2 predicted as lesion

        # Default values if no lesion
        if np.sum(lesion_mask) == 0:
            lesion_volume = 0
            mean_intensity_lesion = 0
            std_intensity_lesion = 0
            max_lesion_diameter = 0
            lesion_centroid_x = lesion_centroid_y = lesion_centroid_z = 0
            lesion_to_pancreas_ratio = 0
            intensity_ratio = 0
            std_ratio = 0
            volume_intensity_interaction = 0
            lesion_range = 0
            lesion_cv = 0
            lesion_found = False
        else:
            lesion_volume = np.sum(lesion_mask)
            mean_intensity_lesion = np.mean(image.get_fdata()[lesion_mask])
            std_intensity_lesion = np.std(image.get_fdata()[lesion_mask])
            max_lesion_diameter = np.ptp(np.argwhere(lesion_mask), axis=0).max()
            lesion_centroid_x, lesion_centroid_y, lesion_centroid_z = np.mean(np.argwhere(lesion_mask), axis=0)
            lesion_to_pancreas_ratio = lesion_volume / (np.sum(pancreas_mask) + 1e-6)
            intensity_ratio = mean_intensity_lesion / (np.mean(image.get_fdata()[pancreas_mask]) + 1e-6)
            std_ratio = std_intensity_lesion / (np.std(image.get_fdata()[pancreas_mask]) + 1e-6)
            volume_intensity_interaction = lesion_volume * mean_intensity_lesion
            lesion_range = np.percentile(image.get_fdata()[lesion_mask], 90) - mean_intensity_lesion
            lesion_cv = std_intensity_lesion / (mean_intensity_lesion + 1e-6)
            lesion_found = True

        # Pancreas stats
        pancreas_volume = np.sum(pancreas_mask)
        mean_intensity_pancreas = np.mean(image.get_fdata()[pancreas_mask])
        std_intensity_pancreas = np.std(image.get_fdata()[pancreas_mask])
        pancreas_cv = std_intensity_pancreas / (mean_intensity_pancreas + 1e-6)
        centroid_distance = np.sqrt(np.mean(np.argwhere(pancreas_mask)[:,0])**2 +
                                    np.mean(np.argwhere(pancreas_mask)[:,1])**2 +
                                    np.mean(np.argwhere(pancreas_mask)[:,2])**2)
        lesion_diameter_ratio = max_lesion_diameter / (pancreas_volume + 1e-6)

        feature_data.append({
            "case_id": case_id,
            "pancreas_volume": pancreas_volume,
            "lesion_volume": lesion_volume,
            "mean_intensity_lesion": mean_intensity_lesion,
            "mean_intensity_pancreas": mean_intensity_pancreas,
            "std_intensity_lesion": std_intensity_lesion,
            "std_intensity_pancreas": std_intensity_pancreas,
            "max_lesion_diameter": max_lesion_diameter,
            "lesion_centroid_x": lesion_centroid_x,
            "lesion_centroid_y": lesion_centroid_y,
            "lesion_centroid_z": lesion_centroid_z,
            "lesion_to_pancreas_ratio": lesion_to_pancreas_ratio,
            "intensity_ratio": intensity_ratio,
            "std_ratio": std_ratio,
            "volume_intensity_interaction": volume_intensity_interaction,
            "lesion_range": lesion_range,
            "pancreas_cv": pancreas_cv,
            "lesion_cv": lesion_cv,
            "lesion_diameter_ratio": lesion_diameter_ratio,
            "centroid_distance": centroid_distance,
            "lesion_found": lesion_found
        })

    except Exception as e:
        print(f"‚ùå Error processing {case_id}: {e}")

# -------------------------
# Save test features
# -------------------------
test_features_df = pd.DataFrame(feature_data)
test_features_df.to_csv(test_features_csv, index=False)
print(f"\nüìå Test features saved to {test_features_csv}")
display(test_features_df.head())

import os
import nibabel as nib
import numpy as np
import pandas as pd
from scipy.ndimage import label as connected_components

# -------------------------
# Paths
# -------------------------
test_images_dir = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/imagesTs"
pred_masks_dir = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/predicted_masks"
test_features_csv = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/test_features.csv"

# -------------------------
# Get test cases
# -------------------------
test_cases = [f.replace("_0000.nii.gz","") for f in os.listdir(test_images_dir) if f.endswith("_0000.nii.gz")]
print(f"üìä Extracting features from {len(test_cases)} test cases...")

feature_data = []

# -------------------------
# Loop over test cases
# -------------------------
for case_id in test_cases:
    image_path = os.path.join(test_images_dir, f"{case_id}_0000.nii.gz")
    mask_path = os.path.join(pred_masks_dir, f"{case_id}_pred_mask.nii.gz")

    if not os.path.exists(image_path):
        print(f"‚ö†Ô∏è Missing image for {case_id}, skipping.")
        continue

    image = nib.load(image_path)
    image_data = image.get_fdata()

    if os.path.exists(mask_path):
        mask = nib.load(mask_path).get_fdata().astype(np.uint8)
        pancreas_mask = (mask == 1)
        lesion_mask = (mask == 2)
    else:
        # If predicted mask does not exist, create dummy masks
        pancreas_mask = np.ones_like(image_data, dtype=bool)
        lesion_mask = np.zeros_like(image_data, dtype=bool)

    # If lesion mask is empty, estimate using intensity threshold inside pancreas
    if np.sum(lesion_mask) == 0:
        pancreas_intensity = image_data[pancreas_mask]
        mean_p = np.mean(pancreas_intensity)
        std_p = np.std(pancreas_intensity)
        threshold = mean_p + 1.0 * std_p  # You can tune 1.0 -> 1.5
        lesion_candidate = (image_data > threshold) & pancreas_mask

        # Remove tiny regions (less than 10 voxels)
        labeled, num_features = connected_components(lesion_candidate)
        lesion_mask = np.zeros_like(image_data, dtype=bool)
        for i in range(1, num_features+1):
            region = (labeled == i)
            if np.sum(region) >= 10:
                lesion_mask |= region

        if np.sum(lesion_mask) == 0:
            # still no lesion, skip case
            print(f"‚ö†Ô∏è No lesion found for {case_id} even after thresholding, skipping.")
            continue

    # -------------------------
    # Extract features
    # -------------------------
    pancreas_volume = np.sum(pancreas_mask)
    lesion_volume = np.sum(lesion_mask)
    mean_intensity_pancreas = np.mean(image_data[pancreas_mask])
    mean_intensity_lesion = np.mean(image_data[lesion_mask])
    std_intensity_pancreas = np.std(image_data[pancreas_mask])
    std_intensity_lesion = np.std(image_data[lesion_mask])
    max_lesion_diameter = np.ptp(np.argwhere(lesion_mask), axis=0)
    lesion_centroid = np.mean(np.argwhere(lesion_mask), axis=0)

    # Derived features
    lesion_to_pancreas_ratio = lesion_volume / (pancreas_volume + 1e-6)
    intensity_ratio = mean_intensity_lesion / (mean_intensity_pancreas + 1e-6)
    std_ratio = std_intensity_lesion / (std_intensity_pancreas + 1e-6)
    volume_intensity_interaction = lesion_volume * mean_intensity_lesion
    lesion_range = np.percentile(image_data[lesion_mask], 90) - mean_intensity_lesion
    pancreas_cv = std_intensity_pancreas / (mean_intensity_pancreas + 1e-6)
    lesion_cv = std_intensity_lesion / (mean_intensity_lesion + 1e-6)
    lesion_diameter_ratio = max_lesion_diameter.max() / (pancreas_volume + 1e-6)
    centroid_distance = np.linalg.norm(lesion_centroid)

    feature_data.append({
        "case_id": case_id,
        "pancreas_volume": pancreas_volume,
        "lesion_volume": lesion_volume,
        "mean_intensity_lesion": mean_intensity_lesion,
        "mean_intensity_pancreas": mean_intensity_pancreas,
        "std_intensity_lesion": std_intensity_lesion,
        "std_intensity_pancreas": std_intensity_pancreas,
        "max_lesion_diameter": max_lesion_diameter.max(),
        "lesion_centroid_x": lesion_centroid[0],
        "lesion_centroid_y": lesion_centroid[1],
        "lesion_centroid_z": lesion_centroid[2],
        "lesion_to_pancreas_ratio": lesion_to_pancreas_ratio,
        "intensity_ratio": intensity_ratio,
        "std_ratio": std_ratio,
        "volume_intensity_interaction": volume_intensity_interaction,
        "lesion_range": lesion_range,
        "pancreas_cv": pancreas_cv,
        "lesion_cv": lesion_cv,
        "lesion_diameter_ratio": lesion_diameter_ratio,
        "centroid_distance": centroid_distance
    })

# -------------------------
# Save test features
# -------------------------
features_df = pd.DataFrame(feature_data)
features_df.to_csv(test_features_csv, index=False)
print(f"\nüìå Test features saved to {test_features_csv}")
display(features_df.head())

import pandas as pd

# -------------------------
# Paths
# -------------------------
test_features_csv = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/test_features.csv"


# Load training features CSV (assuming you have it)
training_csv = "/content/drive/MyDrive/pancreas_project/training_features_expanded.csv"
train_df = pd.read_csv(training_csv)

# Define training_feature_names (exclude label columns)
training_feature_names = [col for col in train_df.columns if col != "subtype"]
training_set_features = set(training_feature_names)

# Load test features
test_df = pd.read_csv(test_features_csv)
X_test = test_df.drop(columns=["case_id"])

# Check which features exist in training but not in test, and vice versa
training_set_features = set(training_feature_names)
test_set_features = set(X_test.columns)

extra_in_training = training_set_features - test_set_features
extra_in_test = test_set_features - training_set_features

print("Extra features in training not in test:", extra_in_training)
print("Extra features in test not in training:", extra_in_test)

# Update training_feature_names to only include features present in test
training_feature_names_aligned = [f for f in training_feature_names if f in X_test.columns]

print("\n‚úÖ Training feature set updated to match test features.")

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score

# -------------------------
# Paths
# -------------------------
training_csv = "/content/drive/MyDrive/pancreas_project/training_features_expanded.csv"
test_csv     = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/test_features.csv"

# -------------------------
# Load data
# -------------------------
train_df = pd.read_csv(training_csv)
test_df  = pd.read_csv(test_csv)

# -------------------------
# Recalculate missing features in training to match test features
# -------------------------
missing_features = [
    "intensity_ratio", "std_ratio", "volume_intensity_interaction",
    "lesion_range", "pancreas_cv", "lesion_cv",
    "lesion_diameter_ratio", "centroid_distance"
]

for feat in missing_features:
    if feat == "intensity_ratio":
        train_df[feat] = train_df["mean_intensity_lesion"] / (train_df["mean_intensity_pancreas"] + 1e-6)
    elif feat == "std_ratio":
        train_df[feat] = train_df["std_intensity_lesion"] / (train_df["std_intensity_pancreas"] + 1e-6)
    elif feat == "volume_intensity_interaction":
        train_df[feat] = train_df["lesion_volume"] * train_df["mean_intensity_lesion"]
    elif feat == "lesion_range":
        train_df[feat] = train_df["max_lesion_diameter"] - train_df["max_lesion_diameter"].min()
    elif feat == "pancreas_cv":
        train_df[feat] = train_df["std_intensity_pancreas"] / (train_df["mean_intensity_pancreas"] + 1e-6)
    elif feat == "lesion_cv":
        train_df[feat] = train_df["std_intensity_lesion"] / (train_df["mean_intensity_lesion"] + 1e-6)
    elif feat == "lesion_diameter_ratio":
        train_df[feat] = train_df["max_lesion_diameter"] / (train_df["pancreas_volume"] + 1e-6)
    elif feat == "centroid_distance":
        train_df[feat] = 0.0  # approximate if centroid info not available

# -------------------------
# Align features between train and test
# -------------------------
features = list(test_df.columns.drop("case_id"))  # exclude case_id
X_train = train_df[features]
X_test  = test_df[features]
y_train = train_df["subtype"]

# -------------------------
# Random Forest with cross-validation and hyperparameter tuning
# -------------------------
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
rf = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=cv,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
best_rf = grid_search.best_estimator_
print(f"\nüìå Best RF Parameters: {grid_search.best_params_}")

# -------------------------
# Predict on training set (for sanity check)
# -------------------------
y_pred_train = best_rf.predict(X_train)
print("\nüìå Training Accuracy:", accuracy_score(y_train, y_pred_train))
print(classification_report(y_train, y_pred_train))

# -------------------------
# Predict on test set
# -------------------------
y_pred_test = best_rf.predict(X_test)
test_df["predicted_subtype"] = y_pred_test

# -------------------------
# Save test predictions
# -------------------------
test_output_path = test_csv.replace(".csv", "_with_predictions.csv")
test_df.to_csv(test_output_path, index=False)
print(f"\n‚úÖ Test predictions saved to {test_output_path}")

import pandas as pd

# -------------------------
# Paths
# -------------------------
test_csv = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/test_features.csv"
output_csv = "/content/drive/MyDrive/pancreas_project/nnUNet_raw/Dataset001_PancreasCancer/subtype_results.csv"

# -------------------------
# Load test features
# -------------------------
test_df = pd.read_csv(test_csv)

# -------------------------
# Align test features with training features
# -------------------------
training_features = best_rf.feature_names_in_  # exact features and order used in training

# Make sure test has all required columns
for col in training_features:
    if col not in test_df.columns:
        test_df[col] = 0  # fill missing columns with zeros

X_test = test_df[training_features]

# -------------------------
# Predict subtypes
# -------------------------
y_test_pred = best_rf.predict(X_test)

# -------------------------
# Save results
# -------------------------
results_df = pd.DataFrame({
    "Names": test_df["case_id"] + ".nii.gz",
    "Subtype": y_test_pred
})

results_df.to_csv(output_csv, index=False)
print(f"‚úÖ Test subtype predictions saved to {output_csv}")